# Infrastructure & Hardware

Choose the right deployment model and hardware for your AI workloads.

## Overview

Decisions about where to deploy AI (edge, on-premises, cloud) and what hardware to use (CPU, GPU, TPU, NPU) have major implications for cost, performance, latency, security, and operational complexity.

## Key Topics

### [Deployment Models](deployment-models.md)
Compare workstation, departmental servers, and cloud options based on:
- Data sensitivity and compliance
- Latency requirements
- Cost structure (CAPEX vs OPEX)
- Operational capabilities

### [Hardware Selection](hardware-selection.md)
When to use CPU, GPU, TPU, or NPU for different AI workloads.

### [Hardware Comparison](hardware-comparison.md)
Detailed comparison of NPU vs GPU vs CPU for latency and power consumption.

### [Edge Inference Hardware](edge-inference-hardware.md)
Hardware recommendations for edge and peripheral inference.

### [Hybrid Architecture](hybrid-architecture.md)
Recommended hybrid architectures combining edge, on-prem, and cloud.
