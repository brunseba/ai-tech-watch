# Glossary

## A

**ABAC (Attribute-Based Access Control)**
: Access control model that uses attributes (user, resource, environment) to make authorization decisions.

**Agentic AI**
: AI systems where models act as autonomous agents that can reason, plan, use tools, and take actions to achieve goals.

**AutoGen**
: Microsoft Research framework for building multi-agent conversational AI systems.

**AWQ (Activation-aware Weight Quantization)**
: Advanced quantization technique that preserves accuracy by being aware of activation distributions.

## C

**CAPEX (Capital Expenditure)**
: Upfront costs for purchasing hardware, software licenses, and infrastructure.

**CrewAI**
: Framework for building role-based multi-agent AI systems where agents collaborate like a crew.

**CPU (Central Processing Unit)**
: General-purpose processor suitable for sequential tasks and light AI workloads.

## D

**DLP (Data Loss Prevention)**
: Security measures to prevent sensitive data from being leaked or exfiltrated.

## E

**Edge AI**
: Running AI models on edge devices (phones, IoT, cameras) rather than in the cloud.

## F

**FP16/FP32**
: Floating-point precision formats (16-bit and 32-bit) used for model weights and activations.

**Fine-tuning**
: Adapting a pre-trained model to a specific task or domain by training on additional data.

## G

**GPU (Graphics Processing Unit)**
: Specialized processor optimized for parallel computations, widely used for AI training and inference.

**Guardrails**
: Safety mechanisms to filter prompts and responses in AI systems to prevent harmful outputs.

## H

**HSM (Hardware Security Module)**
: Physical device that manages and protects cryptographic keys.

**Hybrid Architecture**
: Deployment model combining edge, on-premises, and cloud resources.

## I

**IdP (Identity Provider)**
: System that creates, maintains, and manages identity information (e.g., Keycloak, Okta).

**Inference**
: Running a trained AI model to make predictions on new data.

**INT8**
: 8-bit integer quantization format that reduces model size and increases speed with minimal accuracy loss.

## K

**KMS (Key Management Service)**
: System for managing cryptographic keys (e.g., AWS KMS, HashiCorp Vault).

## L

**LangChain**
: Framework for developing applications powered by language models with chains of operations.

**LangGraph**
: Extension of LangChain for building stateful, multi-actor agent workflows as graphs.

**Latency**
: Time delay between request and response, critical for real-time AI applications.

**LLM (Large Language Model)**
: AI model trained on vast amounts of text data (e.g., GPT, Claude, Llama).

## M

**MFA (Multi-Factor Authentication)**
: Security method requiring two or more verification factors to access a resource.

**MLOps**
: Practices for deploying, monitoring, and maintaining machine learning models in production.

**mTLS (Mutual TLS)**
: Protocol where both client and server authenticate each other using certificates.

## N

**NPU (Neural Processing Unit)**
: Specialized chip designed for efficient neural network inference, common in edge devices.

## O

**OIDC (OpenID Connect)**
: Authentication protocol built on OAuth 2.0 for identity verification.

**Ollama**
: Tool for running LLMs locally with easy model management.

**OPEX (Operating Expenditure)**
: Ongoing operational costs (electricity, maintenance, salaries, subscriptions).

**OPA (Open Policy Agent)**
: Policy engine for unified, declarative policy enforcement across systems.

## P

**PII (Personally Identifiable Information)**
: Data that can identify a specific individual (names, emails, SSNs, etc.).

**PTQ (Post-Training Quantization)**
: Quantizing a model after training without retraining.

## Q

**QAT (Quantization-Aware Training)**
: Training models with quantization in mind to minimize accuracy loss.

**Quantization**
: Reducing model precision (e.g., from 32-bit to 8-bit) to decrease size and increase speed.

## R

**RAG (Retrieval-Augmented Generation)**
: Technique combining information retrieval with generation to ground LLM outputs in facts.

**RBAC (Role-Based Access Control)**
: Access control model that assigns permissions based on user roles.

## S

**Semantic Kernel**
: Microsoft framework for orchestrating AI with plugins and skills.

**Service Mesh**
: Infrastructure layer for handling service-to-service communication (e.g., Istio, Linkerd).

**SIEM (Security Information and Event Management)**
: System that aggregates and analyzes security logs and events.

**SPIFFE/SVID**
: Framework for service identity and attestation in dynamic environments.

**SSO (Single Sign-On)**
: Authentication scheme allowing users to access multiple systems with one set of credentials.

## T

**TCO (Total Cost of Ownership)**
: Complete cost of owning and operating technology over its lifetime (CAPEX + OPEX).

**TGI (Text Generation Inference)**
: HuggingFace toolkit for deploying and serving LLMs efficiently.

**Throughput**
: Number of requests or operations processed per unit of time.

**TLS (Transport Layer Security)**
: Cryptographic protocol for secure communication over networks.

**TPU (Tensor Processing Unit)**
: Google's custom chip optimized for TensorFlow and machine learning operations.

## V

**Vector Database**
: Database optimized for storing and querying high-dimensional vectors (embeddings).

**vLLM**
: High-throughput and memory-efficient inference engine for LLMs.

## Z

**Zero Trust**
: Security model assuming no implicit trust, requiring verification for every access request.

---

*For technical details on any term, consult the relevant section of this documentation.*
