# Enterprise AI Platforms: Deep Comparison

## Overview

This document provides an in-depth analysis of three enterprise AI platforms that target different layers of the AI stack: OPEA (Open Platform for Enterprise AI), Red Hat OpenShift AI, and NVIDIA AI Enterprise.

## Executive Summary

```mermaid
flowchart TB
    subgraph Application["ğŸ¯ Application Layer"]
        OPEA["OPEA<br/>GenAI/RAG Architecture<br/>Microservices & Blueprints"]
    end
    
    subgraph Platform["âš™ï¸ Platform Layer"]
        OSAI["OpenShift AI<br/>MLOps/GenAIOps<br/>Kubernetes-native"]
    end
    
    subgraph Infrastructure["ğŸ”§ Infrastructure Layer"]
        NVAIE["NVIDIA AI Enterprise<br/>GPU-optimized Runtime<br/>CUDA-X & NIM"]
    end
    
    OPEA -->|Runs on| OSAI
    OSAI -->|Accelerated by| NVAIE
    
    style OPEA fill:#e1f5ff
    style OSAI fill:#fff3e0
    style NVAIE fill:#e8f5e9
```

**Key Insight**: These three platforms are complementary rather than competing solutions, operating at different architectural layers.

## Framework Profiles

### OPEA (Open Platform for Enterprise AI)

**Positioning**: Open, vendor-neutral architectural framework for GenAI applications

#### Core Characteristics

**Origin & Governance**
- Linux Foundation AI & Data sandbox project
- Initiated by Intel in 2024
- Community-driven, open-source development model
- Vendor-neutral by design

**Primary Focus**
- Generative AI and RAG (Retrieval-Augmented Generation) pipelines
- Composable microservice architecture
- Multi-provider ecosystem integration
- Reference implementations and blueprints

**Technical Architecture**
```mermaid
flowchart LR
    subgraph OPEA["OPEA Architecture"]
        Gateway["API Gateway<br/>ğŸšª"]
        Megaservice["Megaservices<br/>ğŸ“¦"]
        
        subgraph Microservices["Microservices Layer"]
            LLM["LLM Service"]
            VectorDB["Vector DB"]
            Embedding["Embedding"]
            Prompt["Prompt Engine"]
            Rerank["Reranking"]
        end
        
        Gateway --> Megaservice
        Megaservice --> Microservices
    end
    
    style Gateway fill:#bbdefb
    style Megaservice fill:#c5e1a5
    style Microservices fill:#fff9c4
```

**Key Components**
1. **Microservices**: Single-purpose building blocks (embedding, LLM inference, vector search)
2. **Megaservices**: Composed workflows combining multiple microservices
3. **Gateways**: API orchestration and routing layer
4. **Connectors**: Integration with various AI providers and data sources

**Strengths**
- âœ… Vendor neutrality - not locked to any cloud or hardware vendor
- âœ… Composability - mix and match components
- âœ… Open source - transparent, auditable, community-driven
- âœ… Hardware agnostic - works across CPU, GPU, NPU, TPU
- âœ… Modular design - replace components without rewriting entire stack

**Limitations**
- âš ï¸ Early stage - sandbox project, not production-hardened
- âš ï¸ Limited commercial support - community-driven
- âš ï¸ Integration burden - requires assembly and configuration
- âš ï¸ No managed service - DIY deployment and operations
- âš ï¸ Intel influence - while vendor-neutral, Intel has significant input

**Ideal Use Cases**
- Organizations requiring vendor neutrality
- Multi-cloud or hybrid deployments
- RAG/GenAI application development
- Research and experimentation
- Custom GenAI pipeline requirements

**Maturity Level**: ğŸŸ¡ Emerging (LF AI & Data Sandbox)

---

### Red Hat OpenShift AI

**Positioning**: Enterprise-grade Kubernetes-native AI/ML platform

#### Core Characteristics

**Origin & Governance**
- Red Hat commercial product
- Built on OpenShift (Kubernetes distribution)
- Enterprise support and SLAs
- Part of Red Hat ecosystem

**Primary Focus**
- Complete AI/ML lifecycle management
- MLOps and GenAIOps workflows
- Data science collaboration
- Production model serving
- Hybrid cloud and edge deployment

**Technical Architecture**
```mermaid
flowchart TB
    subgraph OpenShift["OpenShift AI Platform"]
        subgraph Dev["Development"]
            Jupyter["Jupyter/Workbench<br/>ğŸ“Š"]
            VSCode["VS Code"]
            DataSci["Data Science Tools"]
        end
        
        subgraph Training["Training & Tuning"]
            DistTrain["Distributed Training<br/>âš¡"]
            HPO["Hyperparameter Optimization"]
            GPU["GPU Acceleration"]
        end
        
        subgraph Serving["Model Serving"]
            ModelServer["Model Server<br/>ğŸš€"]
            APIGateway["API Gateway"]
            Monitoring["Monitoring"]
        end
        
        subgraph Pipeline["Pipelines"]
            Kubeflow["Kubeflow Pipelines<br/>ğŸ”„"]
            DataPipes["Data Pipelines"]
            MLOps["MLOps Automation"]
        end
        
        Dev --> Training
        Training --> Serving
        Pipeline -.orchestrates.-> Dev
        Pipeline -.orchestrates.-> Training
        Pipeline -.orchestrates.-> Serving
    end
    
    subgraph Infra["OpenShift Infrastructure"]
        K8s["Kubernetes"]
        Storage["Persistent Storage"]
        Network["Networking"]
    end
    
    OpenShift --> Infra
    
    style Dev fill:#e3f2fd
    style Training fill:#f3e5f5
    style Serving fill:#e8f5e9
    style Pipeline fill:#fff3e0
```

**Key Components**
1. **Workbenches**: Jupyter notebooks, VS Code, RStudio for data science
2. **Model Training**: Distributed training, GPU/TPU acceleration
3. **Model Serving**: Production inference with KServe/ModelMesh
4. **Pipelines**: Kubeflow Pipelines for workflow orchestration
5. **Model Registry**: Versioning and lifecycle management
6. **Monitoring**: Observability for models in production

**Strengths**
- âœ… Enterprise support - Red Hat backing with SLAs
- âœ… Kubernetes-native - leverages OpenShift's security and operations
- âœ… Full lifecycle - end-to-end ML/AI workflow coverage
- âœ… Hybrid cloud - consistent experience across on-prem and cloud
- âœ… Security & compliance - built-in RBAC, auditing, certificates
- âœ… Integration - fits into existing Red Hat/OpenShift environments

**Limitations**
- âš ï¸ OpenShift dependency - requires OpenShift as base platform
- âš ï¸ Commercial license - subscription costs
- âš ï¸ Opinionated - specific toolset and workflow patterns
- âš ï¸ Complexity - full platform requires operational expertise
- âš ï¸ Resource intensive - needs significant cluster resources

**Ideal Use Cases**
- Large enterprises already using OpenShift
- Regulated industries (finance, healthcare)
- Organizations requiring enterprise support
- Teams needing MLOps/GenAIOps governance
- Hybrid cloud or multi-cluster deployments

**Maturity Level**: ğŸŸ¢ Production-ready (GA)

---

### NVIDIA AI Enterprise

**Positioning**: GPU-optimized AI infrastructure and microservices suite

#### Core Characteristics

**Origin & Governance**
- NVIDIA commercial product
- Licensed software suite
- Enterprise support from NVIDIA
- Tied to NVIDIA hardware ecosystem

**Primary Focus**
- Maximum GPU performance and utilization
- Optimized AI frameworks and libraries
- Production-ready microservices
- GPU infrastructure management
- Security and stability for production AI

**Technical Architecture**
```mermaid
flowchart TB
    subgraph Apps["Application Layer"]
        GenAI["Generative AI<br/>NIM Microservices"]
        CV["Computer Vision"]
        Speech["Speech AI"]
        RecSys["Recommender Systems"]
    end
    
    subgraph Frameworks["AI Frameworks Layer"]
        NeMo["NeMo<br/>(LLM Training)"]
        TensorRT["TensorRT<br/>(Inference)"]
        Triton["Triton Server<br/>(Model Serving)"]
        RAPIDS["RAPIDS<br/>(Data Science)"]
    end
    
    subgraph Runtime["Runtime & Libraries"]
        CUDAX["CUDA-X Libraries<br/>cuDNN, cuBLAS, etc."]
        NCCL["NCCL<br/>(Multi-GPU)"]
        TensorCore["Tensor Core Ops"]
    end
    
    subgraph Infra["Infrastructure"]
        Drivers["NVIDIA Drivers"]
        K8sOp["GPU Operator"]
        NetworkOp["Network Operator"]
        GPUs["NVIDIA GPUs<br/>A100, H100, L40S"]
    end
    
    Apps --> Frameworks
    Frameworks --> Runtime
    Runtime --> Infra
    
    style Apps fill:#e8f5e9
    style Frameworks fill:#fff3e0
    style Runtime fill:#f3e5f5
    style Infra fill:#e3f2fd
```

**Key Components**
1. **CUDA-X Libraries**: Optimized libraries (cuDNN, cuBLAS, cuSPARSE)
2. **NIM (NVIDIA Inference Microservices)**: Prebuilt inference containers
3. **NeMo**: Framework for training large language models
4. **Triton Inference Server**: Production model serving
5. **TensorRT**: Inference optimization and acceleration
6. **RAPIDS**: GPU-accelerated data science
7. **GPU Operator**: Kubernetes GPU management

**Strengths**
- âœ… Performance - maximum GPU utilization and throughput
- âœ… Optimization - highly tuned libraries and frameworks
- âœ… Enterprise support - NVIDIA backing with SLAs
- âœ… Validated stacks - tested hardware/software combinations
- âœ… Security updates - CVE patches and security hardening
- âœ… Production-ready - battle-tested microservices

**Limitations**
- âš ï¸ NVIDIA lock-in - requires NVIDIA GPUs
- âš ï¸ License costs - commercial software with per-GPU licensing
- âš ï¸ Hardware dependency - tied to specific GPU generations
- âš ï¸ Vendor-specific - not portable to AMD, Intel accelerators
- âš ï¸ Complexity - many components and integration points

**Ideal Use Cases**
- NVIDIA GPU-based infrastructure
- Performance-critical AI workloads
- Organizations requiring maximum ROI on GPU investment
- Production GenAI deployments
- Large-scale inference serving

**Maturity Level**: ğŸŸ¢ Production-ready (GA)

---

## Detailed Comparison Matrix

### Strategic Positioning

| Dimension | OPEA | OpenShift AI | NVIDIA AI Enterprise |
|-----------|------|--------------|---------------------|
| **Business Model** | Open source community | Commercial subscription | Licensed software |
| **Target Customer** | Developers, architects | Enterprise IT | GPU infrastructure owners |
| **Lock-in Risk** | ğŸŸ¢ Low | ğŸŸ¡ Medium (OpenShift) | ğŸ”´ High (NVIDIA GPUs) |
| **Total Cost** | ğŸŸ¢ Low (DIY) | ğŸŸ¡ Medium (subscription) | ğŸ”´ High (license + GPU) |
| **Time to Value** | ğŸŸ¡ Medium (assembly) | ğŸŸ¢ Fast (integrated) | ğŸŸ¢ Fast (optimized) |

### Technical Capabilities

| Capability | OPEA | OpenShift AI | NVIDIA AI Enterprise |
|-----------|------|--------------|---------------------|
| **GenAI/RAG** | ğŸŸ¢ Primary focus | ğŸŸ¡ Supported | ğŸŸ¢ NIM microservices |
| **Model Training** | ğŸ”´ Not included | ğŸŸ¢ Distributed training | ğŸŸ¢ NeMo framework |
| **Model Serving** | ğŸŸ¡ Via components | ğŸŸ¢ KServe/ModelMesh | ğŸŸ¢ Triton Server |
| **MLOps/Pipelines** | ğŸ”´ Not included | ğŸŸ¢ Kubeflow Pipelines | ğŸŸ¡ Partner solutions |
| **Data Science Tools** | ğŸ”´ Not included | ğŸŸ¢ Notebooks/IDE | ğŸŸ¡ RAPIDS |
| **Observability** | ğŸ”´ Not included | ğŸŸ¢ Built-in monitoring | ğŸŸ¡ Partner integration |

### Operational Characteristics

| Aspect | OPEA | OpenShift AI | NVIDIA AI Enterprise |
|--------|------|--------------|---------------------|
| **Deployment Model** | Container/K8s | OpenShift cluster | Any K8s + NVIDIA GPUs |
| **Multi-cloud** | ğŸŸ¢ Excellent | ğŸŸ¢ Good (hybrid) | ğŸŸ¢ Cloud-agnostic |
| **Edge Support** | ğŸŸ¢ Yes | ğŸŸ¢ Yes | ğŸŸ¢ Yes (with NVIDIA Edge) |
| **Air-gapped** | ğŸŸ¢ Possible | ğŸŸ¢ Supported | ğŸŸ¢ Supported |
| **Upgrade Complexity** | ğŸŸ¡ Component-by-component | ğŸŸ¢ Integrated updates | ğŸŸ¡ Coordinated updates |

### Hardware & Acceleration

| Hardware Type | OPEA | OpenShift AI | NVIDIA AI Enterprise |
|--------------|------|--------------|---------------------|
| **NVIDIA GPU** | âœ… Supported | âœ… Supported | âœ… **Optimized** |
| **AMD GPU** | âœ… Possible | âœ… Possible | âŒ Not supported |
| **Intel GPU** | âœ… **Optimized** | âœ… Supported | âŒ Not supported |
| **CPU-only** | âœ… Works | âœ… Works | âš ï¸ Limited value |
| **NPU/TPU** | âœ… Extensible | âš ï¸ Limited | âŒ Not supported |

### Support & Ecosystem

| Aspect | OPEA | OpenShift AI | NVIDIA AI Enterprise |
|--------|------|--------------|---------------------|
| **Vendor Support** | Community | Red Hat Enterprise | NVIDIA Enterprise |
| **SLA Availability** | âŒ No | âœ… Yes | âœ… Yes |
| **Training/Certs** | ğŸ”´ Limited | ğŸŸ¢ Extensive | ğŸŸ¢ Available |
| **Partner Ecosystem** | ğŸŸ¡ Growing | ğŸŸ¢ Mature (Red Hat) | ğŸŸ¢ Mature (NVIDIA) |
| **Community Size** | ğŸŸ¡ Small (new) | ğŸŸ¢ Large | ğŸŸ¢ Large |

---

## Integration Patterns

### Pattern 1: Full Stack Integration

**Architecture**: OPEA on OpenShift AI with NVIDIA AI Enterprise

```mermaid
flowchart TB
    subgraph App["Application Layer"]
        OPEA_App["OPEA GenAI App<br/>RAG Microservices"]
    end
    
    subgraph Platform["Platform Layer"]
        OSAI_Platform["OpenShift AI<br/>MLOps + Pipelines"]
    end
    
    subgraph Runtime["Runtime Layer"]
        NVAIE_Runtime["NVIDIA AI Enterprise<br/>Triton + NIM + CUDA-X"]
    end
    
    subgraph Infra["Infrastructure"]
        GPU["NVIDIA A100/H100 GPUs"]
        Storage["High-perf Storage"]
        Network["InfiniBand/RoCE"]
    end
    
    OPEA_App -->|Deployed on| OSAI_Platform
    OSAI_Platform -->|Accelerated by| NVAIE_Runtime
    NVAIE_Runtime -->|Runs on| Infra
    
    style App fill:#e1f5ff
    style Platform fill:#fff3e0
    style Runtime fill:#e8f5e9
    style Infra fill:#f3e5f5
```

**Benefits**:
- ğŸ¯ **OPEA** defines the GenAI application architecture
- âš™ï¸ **OpenShift AI** provides MLOps, security, and governance
- ğŸš€ **NVIDIA AI Enterprise** maximizes performance and GPU utilization

**Considerations**:
- Highest cost (3 license/subscription models)
- Maximum capabilities and support
- Best performance and enterprise features
- Complex integration and operational overhead

---

### Pattern 2: Platform + Runtime

**Architecture**: OpenShift AI with NVIDIA AI Enterprise (no OPEA)

**Use Case**: Traditional ML/AI workflows without specific GenAI/RAG focus

**Benefits**:
- Production-ready platform with MLOps
- Maximum GPU performance
- Enterprise support from both vendors

**Trade-offs**:
- No pre-built GenAI architectural patterns
- Must architect RAG/GenAI applications from scratch

---

### Pattern 3: Open Stack

**Architecture**: OPEA on vanilla Kubernetes (no OpenShift AI or NVAIE)

**Use Case**: Cost-conscious, cloud-native deployments

**Benefits**:
- Minimal licensing costs
- Maximum flexibility
- Vendor neutrality

**Trade-offs**:
- DIY MLOps and operations
- Limited enterprise support
- Must build tooling and governance

---

### Pattern 4: GPU-Optimized

**Architecture**: NVIDIA AI Enterprise only (no OPEA or OpenShift AI)

**Use Case**: GPU-intensive inference workloads, NIM deployment

**Benefits**:
- Maximum GPU performance
- Lowest complexity
- Pre-built inference microservices

**Trade-offs**:
- No MLOps platform
- Limited to NVIDIA ecosystem
- Must build orchestration layer

---

## Decision Framework

### Selection Criteria

```mermaid
flowchart TD
    Start["Enterprise AI<br/>Platform Selection"]
    
    Q1{"Already using<br/>OpenShift?"}
    Q2{"NVIDIA GPU<br/>infrastructure?"}
    Q3{"Need GenAI/RAG<br/>architecture?"}
    Q4{"Require vendor<br/>neutrality?"}
    Q5{"Need enterprise<br/>support?"}
    
    OSAI["âœ… OpenShift AI"]
    NVAIE["âœ… NVIDIA AI Enterprise"]
    OPEA["âœ… OPEA"]
    Full["âœ… All Three<br/>(Full Stack)"]
    OSAIplus["âœ… OpenShift AI<br/>+ NVIDIA AIE"]
    OPEAonly["âœ… OPEA<br/>(Open Stack)"]
    
    Start --> Q1
    Q1 -->|Yes| Q2
    Q1 -->|No| Q4
    
    Q2 -->|Yes| Q3
    Q2 -->|No| OSAI
    
    Q3 -->|Yes| Full
    Q3 -->|No| OSAIplus
    
    Q4 -->|Yes| Q5
    Q4 -->|No| NVAIE
    
    Q5 -->|Yes| OSAI
    Q5 -->|No| OPEAonly
    
    style Full fill:#c8e6c9
    style OSAI fill:#fff9c4
    style NVAIE fill:#b3e5fc
    style OPEA fill:#f8bbd0
```

### By Organization Type

| Organization Type | Recommended Stack | Rationale |
|-------------------|-------------------|-----------|
| **Large Enterprise (Red Hat shop)** | OpenShift AI + NVIDIA AIE | Leverages existing investments, enterprise support |
| **Large Enterprise (NVIDIA GPUs)** | NVIDIA AIE + OPEA | Maximum ROI on GPU investment, GenAI patterns |
| **Startup/SMB** | OPEA on managed K8s | Low cost, flexibility, cloud-native |
| **Regulated Industry** | OpenShift AI (full stack) | Compliance, audit trails, enterprise security |
| **AI Research Lab** | OPEA + NVIDIA AIE | Flexibility + performance, no platform constraints |
| **Multi-cloud Enterprise** | OpenShift AI | Consistent hybrid cloud experience |

### By Use Case

| Use Case | Primary Platform | Secondary Components |
|----------|-----------------|---------------------|
| **RAG/GenAI Applications** | OPEA | + OpenShift AI for production |
| **ML Model Training** | OpenShift AI | + NVIDIA AIE for GPU optimization |
| **High-throughput Inference** | NVIDIA AIE | + OPEA for orchestration |
| **MLOps/Model Governance** | OpenShift AI | + NVIDIA AIE for acceleration |
| **Multi-vendor Flexibility** | OPEA | + Managed K8s service |
| **Edge AI Deployment** | All three | Distributed architecture |

---

## Cost Analysis

### Total Cost of Ownership (3-Year Projection)

**Assumptions**: 
- 10-node GPU cluster (8x A100 GPUs per node = 80 GPUs)
- 100 data scientists/ML engineers
- Production GenAI applications

| Cost Category | OPEA Only | OpenShift AI Only | NVIDIA AIE Only | Full Stack |
|--------------|-----------|-------------------|-----------------|------------|
| **Software Licenses** | $0 | ~$500K | ~$800K | ~$1.3M |
| **Hardware (GPUs)** | $2.4M | $2.4M | $2.4M | $2.4M |
| **Professional Services** | $300K | $200K | $200K | $400K |
| **Training** | $50K | $100K | $100K | $150K |
| **Support** | $0 | $150K | $240K | $390K |
| **Operational Overhead** | $600K | $300K | $400K | $350K |
| **Total (3yr)** | **$3.35M** | **$3.65M** | **$4.14M** | **$4.99M** |

**Key Insights**:
- OPEA is lowest cost but highest operational burden
- Full stack has highest license costs but lowest operational overhead
- OpenShift AI provides balance of cost and support

---

## Technology Maturity & Roadmap

### Maturity Assessment

| Platform | Release Status | Production Readiness | Community Momentum |
|----------|---------------|---------------------|-------------------|
| **OPEA** | Sandbox (2024) | ğŸŸ¡ Experimental | ğŸŸ¢ Growing fast |
| **OpenShift AI** | GA (2023+) | ğŸŸ¢ Production | ğŸŸ¢ Stable |
| **NVIDIA AIE** | GA (2022+) | ğŸŸ¢ Production | ğŸŸ¢ Mature |

### Future Outlook (2025-2026)

**OPEA**
- Expected graduation from LF AI & Data sandbox
- Broader vendor adoption and contributions
- Integration with more cloud providers
- Enhanced security and governance features

**OpenShift AI**
- Deeper GenAI/LLM capabilities
- Multi-model governance
- Enhanced observability for AI workloads
- Edge AI orchestration improvements

**NVIDIA AI Enterprise**
- Expanded NIM microservice catalog
- Support for new GPU architectures (B100)
- Enhanced multi-tenant isolation
- Integration with more enterprise platforms

---

## Security & Compliance Considerations

### Security Posture

| Security Aspect | OPEA | OpenShift AI | NVIDIA AIE |
|----------------|------|--------------|------------|
| **Supply Chain** | ğŸŸ¡ Community | ğŸŸ¢ Red Hat verified | ğŸŸ¢ NVIDIA signed |
| **Vulnerability Mgmt** | ğŸŸ¡ Community | ğŸŸ¢ CVE tracking | ğŸŸ¢ CVE tracking |
| **Access Control** | âš ï¸ DIY | ğŸŸ¢ RBAC built-in | ğŸŸ¡ K8s-dependent |
| **Secrets Management** | âš ï¸ DIY | ğŸŸ¢ Vault integration | ğŸŸ¡ K8s-dependent |
| **Audit Logging** | âš ï¸ DIY | ğŸŸ¢ Comprehensive | ğŸŸ¡ Limited |
| **Compliance Certs** | âŒ None | ğŸŸ¢ SOC2, ISO | ğŸŸ¢ Multiple |

### Compliance Support

**OpenShift AI** leads in compliance:
- FIPS 140-2 validated cryptography
- Common Criteria certification
- FedRAMP authorized
- HIPAA compliance support

**NVIDIA AI Enterprise** provides:
- Security updates for 3+ years
- CVE patching for frameworks
- Validated container images

**OPEA** requires:
- Self-assessment and hardening
- Third-party security scanning
- Custom compliance implementation

---

## Recommendations

### For Immediate Adoption

**Choose OpenShift AI if**:
- âœ… Already using Red Hat/OpenShift ecosystem
- âœ… Need enterprise support and SLAs
- âœ… Require comprehensive MLOps platform
- âœ… Operating in regulated industry
- âœ… Have budget for commercial platform

**Choose NVIDIA AI Enterprise if**:
- âœ… Have significant NVIDIA GPU investment
- âœ… Performance is critical success factor
- âœ… Need production-ready inference microservices
- âœ… Want maximum ROI on GPU infrastructure
- âœ… Require vendor support for AI stack

**Choose OPEA if**:
- âœ… Building GenAI/RAG applications
- âœ… Require vendor neutrality and flexibility
- âœ… Have strong engineering team for DIY approach
- âœ… Want to avoid vendor lock-in
- âœ… Experimental or research workloads

### For Combined Strategy

**Recommended Approach**: Phased adoption

**Phase 1** (0-6 months): Foundation
- Deploy OpenShift AI for MLOps platform
- Add NVIDIA AI Enterprise if using NVIDIA GPUs
- Establish governance and security

**Phase 2** (6-12 months): GenAI Layer
- Evaluate OPEA for GenAI/RAG applications
- Pilot OPEA microservices on OpenShift AI
- Compare with custom-built solutions

**Phase 3** (12+ months): Optimization
- Optimize based on actual usage patterns
- Consolidate or expand based on needs
- Re-evaluate vendor relationships

---

